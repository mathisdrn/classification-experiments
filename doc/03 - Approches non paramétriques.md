---
title: Approches non paramÃ©triques
---
## Algorithme des k plus proches voisins
L'algorithme k-Nearest Neighbors (KNN) est une meÌthode d'apprentissage superviseÌ utiliseÌe pour la classification et la reÌgression. Il est baseÌ sur le principe que des objets proches dans lâ€™espace des caracteÌristiques ont tendance aÌ€ appartenir aÌ€ la meÌ‚me classe ou aÌ€ avoir des valeurs similaires.

KNN fonctionne en attribuant une classe aÌ€ un point inconnu en fonction des voisins les plus proches dans lâ€™ensemble dâ€™entraiÌ‚nement. La classification se fait par vote majoritaire (la classe la plus freÌquente parmi les voisins), tandis que la reÌgression prend geÌneÌralement la moyenne des valeurs des voisins.

Principe de fonctionnement :

- 1.Initialisation :

SeÌlectionner aleÌatoirement  centroiÌˆdes parmi les points de donneÌes.

- 2. Assignation des points aux clusters :

Chaque point est affecteÌ au cluster dont le centroiÌˆde est le plus proche, en utilisant la distance euclidienne :

$$d(x_i, c_j) = \sqrt{\sum_{d=1}^{D} (x_{id} - c_{jd})^2}$$

- $x_i$ est un point de donneÌes.
 - $c_j$ est le centroiÌˆde du cluster ğŸ‘¦jğŸ‘¦.
 - $D$ est le nombre de dimensions des donneÌes.

3. Mise aÌ€ jour des centroiÌˆdes :

Une fois tous les points assigneÌs, chaque centroiÌˆde est recalculeÌ comme la moyenne des points de son cluster :


$$c_j = \frac{1}{N_j} \sum_{x_i \in C_j} x_i$$

- $N_j$ est le nombre de points appartenant au cluster $C_j$.

Les eÌtapes (2) Assignation et (3) Mise aÌ€ jour sont reÌpeÌteÌes jusqu'aÌ€ convergence, c'est-aÌ€-dire : Les centroiÌˆdes ne changent plus ou un criteÌ€re d'arreÌ‚t est atteint (ex. un nombre maximal d'iteÌrations).


Avantages :

- Simple aÌ€ comprendre et aÌ€ impleÌmenter.
- Aucune phase dâ€™apprentissage : lâ€™algorithme sâ€™adapte directement aux nouvelles donneÌes.
- Performant sur des jeux de donneÌes de petite taille et bien seÌparables.


InconveÌnients :

- Sensible au choix de et aÌ€ la meÌtrique de distance.
- Lourd en calcul pour de grands ensembles de donneÌes, car il neÌcessite le calcul des distances aÌ€ chaque preÌdiction.
- Moins performant si les donneÌes contiennent beaucoup de dimensions non pertinentes.


## Distance-Weighted k-Nearest Neighbors (DW-KNN)

L'algorithme Distance-Weighted k-Nearest Neighbors (DW-KNN) est une variante du k-Nearest Neighbors (KNN).

DW-KNN fonctionne en attribuant un poids aux voisins les plus proches en fonction de leur distance au point aÌ€ preÌdire. Contrairement au KNN classique, ouÌ€ chaque voisin contribue eÌgalement aÌ€ la deÌcision finale, DW-KNN accorde une importance plus grande aux voisins les plus proches en utilisant une fonction de pondeÌration baseÌe sur la distance, comme l'inverse de la distance () ou une fonction gaussienne:
$$w_i = \frac{1}{d(x, x_i)^\beta}$$

 $d(x, x_i)$ est la distance entre le point Ã  prÃ©dire et son voisin.

 $\beta$ est un paramÃ¨tre qui contrÃ´le lâ€™importance de la distance.



Avantages :

- Donne plus d'importance aux voisins les plus proches, reÌduisant l'impact des points eÌloigneÌs.
- AmeÌliore la preÌcision dans les zones ouÌ€ les classes sont meÌlangeÌes.
- Peut mieux geÌrer les donneÌes bruiteÌes par rapport au KNN classique.


InconveÌnients :

- Plus couÌ‚teux en calcul, car il neÌcessite le calcul des poids pour chaque voisin.
- Sensible au choix de la meÌtrique de distance et au parameÌ€tre .
- Moins efficace pour les ensembles de donneÌes treÌ€s grands en raison du recalcul des distances pour chaqueÂ preÌdiction.

## Algorithme des plus proches voisins condensÃ©s:

L'algorithme Condensed Nearest Neighbor (CNN) est une autre variante du KNN utiliseÌe pour reÌduire la taille de lâ€™ensemble dâ€™entraiÌ‚nement tout en conservant une bonne preÌcision de classification. Il fait partie des meÌthodes de reÌduction de prototypes qui visent aÌ€ alleÌger le couÌ‚t de stockage et de calcul de KNN en seÌlectionnant un sous-ensemble repreÌsentatif des donneÌes.

CNN fonctionne en construisant un ensemble de prototypes $S$ aÌ€ partir des donneÌes dâ€™entraiÌ‚nement. Il commence avec un eÌchantillon minimal et ajoute uniquement les points qui sont mal classeÌs par cet eÌchantillon. Ainsi, seule une fraction des donneÌes est conserveÌe, ce qui permet dâ€™acceÌleÌrer les preÌdictions sans trop affecter la preÌcision.

Avantages :

- ReÌduit consideÌrablement le nombre de points de reÌfeÌrence, ameÌliorant l'efficaciteÌ de KNN.
- Diminue le temps de calcul et lâ€™espace meÌmoire requis.
- Permet dâ€™eÌliminer les redondances dans lâ€™ensemble dâ€™entraiÌ‚nement.


InconveÌnients :

- Peut ne pas bien fonctionner si les classes sont fortement entremeÌ‚leÌes.
- La phase de reÌduction peut eÌ‚tre couÌ‚teuse en temps de calcul.
- Risque de perdre des informations importantes si les prototypes seÌlectionneÌs ne sont pas bienÂ repreÌsentatifs.

## Locally Adaptive k-Nearest Neighbors (LA-KNN)


L'algorithme Locally Adaptive k-Nearest Neighbors (LA-KNN) est aussi une autre variante du KNN qui adapte dynamiquement le nombre de voisins en fonction des caracteÌristiques locales des donneÌes. Contrairement au KNN classique ouÌ€ est fixeÌ pour toutes les observations, LA-KNN ajuste en fonction de la densiteÌ locale des points, permettant une meilleure flexibiliteÌ dans les reÌgions ouÌ€ les classes sont plus difficiles aÌ€ seÌparer.

LA-KNN fonctionne en augmentant dans les zones de faible densiteÌ (ouÌ€ les points sont plus disperseÌs) et en le reÌduisant dans les zones denses pour eÌviter les erreurs de classification dues au bruit. Lâ€™adaptation peut se faire aÌ€ lâ€™aide de meÌtriques comme lâ€™entropie locale ou la variance des distances entre les voisins.

Avantages :

- AmeÌliore la robustesse du KNN dans les zones de densiteÌ variable.
- ReÌduit le risque dâ€™overfitting dans les zones treÌ€s peupleÌes et dâ€™underfitting dans les zones clairsemeÌes.
- Peut mieux geÌrer les classes deÌseÌquilibreÌes en ajustant selon la distribution locale.


InconveÌnients :

- Plus complexe aÌ€ impleÌmenter que le KNN classique.
- NeÌcessite un calcul suppleÌmentaire pour ajuster , ce qui augmente le temps dâ€™exeÌcution.
- Sensible au choix de la meÌthode dâ€™adaptation et aÌ€ la meÌtrique de distanceÂ utiliseÌe.
