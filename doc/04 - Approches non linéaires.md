---
title: Approches non linÃ©aires
---

## Decision Tree

L'algorithme des arbres de deÌcision (Decision Tree) est une meÌthode d'apprentissage superviseÌ utiliseÌe pour la classification et la reÌgression. Il repose sur une structure arborescente ouÌ€ chaque nÅ“ud repreÌsente une question sur une caracteÌristique des donneÌes, chaque branche correspond aÌ€ une reÌponse possible, et chaque feuille donne une preÌdiction.

:::{image} ./../assets/image.png
:width: 550px
:alt: decisiontree

:::
### Principe de fonctionnement

1. CreÌation de l'arbre : L'algorithme divise les donneÌes en fonction de la caracteÌristique qui maximise la seÌparation des classes ou minimise l'erreur de preÌdiction. Ce choix se fait souvent avec des mesures comme l'entropie (pour l'indice de Gini ou l'information gain en classification) ou l'erreur quadratique moyenne (en reÌgression).

2. Parcours de l'arbre : Pour preÌdire une nouvelle observation, on part de la racine et on suit les branches en fonction des valeurs des caracteÌristiques jusqu'aÌ€ atteindre une feuille.

3. PreÌdiction :
    
    - Classification : L'eÌtiquette de la classe la plus freÌquente dans la feuille est assigneÌe.
    
    - ReÌgression : La valeur moyenne des observations contenues dans la feuille est utiliseÌe comme preÌdiction.

### CriteÌ€res de division en classification

#### 1.Entropie

Lâ€™entropie mesure lâ€™homogeÌneÌiteÌ dâ€™un ensemble. Elle est deÌfinie par :
$$ H(S) = - p_1 \log_2 p_1 - p_2 \log_2 p_2$$

ouÌ€ :

 et $ p_i $ sont les proportions des classes dans lâ€™ensemble .

Une entropie de 0 signifie que tous les eÌleÌments appartiennent aÌ€ une seule classe (ensemble pur).

Une entropie de 1 signifie que les classes sont reÌparties de manieÌ€re eÌgale (ensemble totalement incertain).


Le gain dâ€™information () est utiliseÌ pour choisir la meilleure caracteÌristique qui permet la meilleu sÃ©paration des classes en mesurant la rÃ©duction d'entropie aprÃ¨s chaque division, il est donnÃ© par la formule suivante:

$$ IG(S, A) = H(S) - \sum_{v \in V} \frac{|S_v|}{|S|} H(S_v) $$

ouÌ€ $S_v$ est le sous-ensemble des donneÌes ayant la valeur $v$ pour lâ€™attribut .

#### 2. Indice de Gini

L'indice de Gini est une alternative aÌ€ l'entropie et mesure lâ€™impureteÌ dâ€™un ensemble :

$$ Gini(S) = 1 - p_1^2 - p_2^2 $$

ouÌ€ $p_i$ et sont les proportions des classes dans lâ€™ensemble .
Lâ€™indice de Gini est minimal (0) lorsque lâ€™ensemble est homogeÌ€ne et maximal (0.5 en classification binaire) lorsque les classes sont eÌquilibreÌes.

De la meÌ‚me manieÌ€re, on peut calculer la reÌduction dâ€™impureteÌ(gain dâ€™information) avec lâ€™indice de Gini :

$$ \Delta Gini = Gini(S) - \sum_{v \in V} \frac{|S_v|}{|S|} Gini(S_v)$$

## ForÃªt AlÃ©atoire 
:::{image} ./../assets/randomforest.png
:width: 550px
:alt: randomforest
:::
Lâ€™algorithme ForÃªt AlÃ©atoire(Random Forest en anglais) est une extension des arbres de deÌcision, utiliseÌe pour la classification et la reÌgression. Il repose sur un principe dâ€™apprentissage ensembliste (ensemble learning), combinant plusieurs arbres pour ameÌliorer la robustesse et la preÌcision des preÌdictions.

### 1. Construction d'une forÃªt d'arbres de dÃ©cision

L'algorithme gÃ©nÃ¨re plusieurs arbres en effectuant un Ã©chantillonnage alÃ©atoire avec remise sur le jeu de donnÃ©es (bootstrap).

Ã€ chaque division dans un arbre, un sous-ensemble alÃ©atoire de caractÃ©ristiques est sÃ©lectionnÃ© pour Ã©viter que tous les arbres se construisent de la mÃªme maniÃ¨re.

### 2. PrÃ©diction par agrÃ©gation des arbres

- Classification : Chaque arbre vote pour une classe, et la classe majoritaire est retenue (vote majoritaire).
- RÃ©gression : La prÃ©diction finale est la moyenne des prÃ©dictions des diffÃ©rents arbres.

### Bootstrap
 

Le bootstrap est une meÌthode de reÌeÌchantillonnage avec remise qui creÌe plusieurs sous-ensembles aÌ€ partir dâ€™un meÌ‚me jeu de donneÌes. Dans Random Forest, il est utiliseÌ pour entraiÌ‚ner chaque arbre sur un eÌchantillon aleÌatoire, introduisant de la diversiteÌ et reÌduisant le sur-apprentissage. Cette technique ameÌliore la robustesse et la geÌneÌralisationÂ duÂ modeÌ€le.
### Avantages

- Plus prÃ©cis et gÃ©nÃ©ralisable sans risque de surapprentissage qu'un arbre unique
- Peu sensible aux valeurs aberrantes grÃ¢ce Ã  l'agrÃ©gation des arbres
- Robustesse : Une petite variation dans les donnÃ©es n'impacte pas fortement le modÃ¨le
- GÃ¨re bien les donnÃ©es manquantes et les grands ensembles de donnÃ©es

### InconvÃ©nients

- Moins interprÃ©table qu'un arbre unique
- Temps de calcul plus long pour de grands ensembles de donnÃ©es
- Performances limitÃ©es sur des donnÃ©es trÃ¨s haute dimension

## ForÃªt AlÃ©atoire Sensible au CoÃ»t

Lâ€™algorithme ForÃªt AlÃ©atoire Sensible au CoÃ»t (Cost-Sensitive Random Forest en anglais) est une adaptation du Random Forest concÌ§ue pour prendre en compte des couÌ‚ts diffeÌrents dâ€™erreur lors de la classification. Il est particulieÌ€rement utile lorsque les classes sont deÌseÌquilibreÌes ou que certaines erreurs ont un impact plus important que dâ€™autres, comme en dÃ©tection de fraude ou en diagnostic mÃ©dical.

### Formule du Gini pondÃ©rÃ© par le coÃ»t

$\text{Gini}_C(D) = \sum_{i=1}^{C} p_i \sum_{j=1}^{C} C_{ij} p_j$

OÃ¹:
- $C_{ij}$ reprÃ©sente le coÃ»t associÃ© Ã  la classification erronÃ©e d'une observation de classe $i$ en classe $j$
- $C_{ii} = 0$ (pas de coÃ»t pour une classification correcte)
- $C_{ij} > 0$ pour $i \neq j$ (coÃ»t positif pour une erreur)

Principe de fonctionnement :

1. PondeÌration des erreurs

Contrairement au Random Forest classique, qui minimise simplement le taux dâ€™erreur global, le Cost-Sensitive Random Forest attribue un couÌ‚t aÌ€ chaque type dâ€™erreur.

Une matrice de couÌ‚ts est deÌfinie pour peÌnaliser diffeÌremment les erreurs en fonction de leur importance.

2. Construction dâ€™une foreÌ‚t de deÌcision pondeÌreÌe

Comme dans Random Forest, plusieurs arbres sont geÌneÌreÌs sur des eÌchantillons bootstrap des donneÌes.

AÌ€ chaque division, le criteÌ€re de seÌlection (comme lâ€™indice de Gini ou lâ€™entropie) est ajusteÌ pour tenir compte des couÌ‚ts des erreurs.

$$ Gini_C(D) = \sum_{i=1}^{C} p_i \sum_{j=1}^{C} C_{ij} p_j $$ 

Le terme $ C_{ij}$ reprÃ©sente le coÃ»t associÃ© Ã  la classification erronÃ©e d'une observation appartenant Ã  la classe $ğ‘–$ en classe $ğ‘—$.

$C_{ii}=0$ : Il n'y a aucun coÃ»t lorsqu'une observation est correctement classÃ©e.

$ C_{ij} > 0$ pour $ğ‘–â‰ ğ‘—$: Il y a un coÃ»t lorsqu'une observation de la classe $ğ‘–$ est incorrectement classÃ©e comme appartenant Ã  la classe $ğ‘—$.

3. PreÌdiction avec prise en compte des couÌ‚ts

 PlutoÌ‚t que dâ€™utiliser un vote majoritaire simple entre les arbres, la classe preÌdite est celle qui minimise le couÌ‚t dâ€™erreur attendu.Cette modification rÃ©duit le biais envers les classes majoritaires et amÃ©liore la prise en compte des classes minoritaires.

### Avantages

- Meilleure prise en compte des deÌseÌquilibres de classe: Dans un probleÌ€me ouÌ€ une classe est beaucoup moins freÌquente que lâ€™autre, Random Forest peut privileÌgier la classe majoritaire. En inteÌgrant un couÌ‚t plus eÌleveÌ pour les erreurs sur la classe minoritaire, lâ€™algorithme devient plus eÌquitable.
- Plus adapteÌ aux contextes ouÌ€ certaines erreurs couÌ‚tent plus cher que dâ€™autres : Par exemple, dans une deÌtection de fraude bancaire, une fausse alerte (preÌdire une fraude inexistante) est moins grave quâ€™un faux neÌgatif (ne pas deÌtecter une fraude reÌelle).
- Garde la robustesse du Random Forest tout en ameÌliorant la gestion des erreurs critiques.

### InconvÃ©nients

- NeÌcessite de bien deÌfinir la matrice de couÌ‚ts, ce qui peut eÌ‚tre deÌlicat en lâ€™absence dâ€™informations preÌcises sur lâ€™impact des erreurs.
- Peut eÌ‚tre plus long aÌ€ entraiÌ‚ner, car lâ€™arbre doit ajuster ses criteÌ€res de seÌlection en fonction des couÌ‚ts dâ€™erreur.
- Moins intuitif que le Random Forest standard, car les deÌcisions ne sont plus baseÌes uniquement sur des votes majoritaires.

## AdaBoost (Adaptive Boosting)

L'algorithme AdaBoost (Adaptive Boosting) est une meÌthode dâ€™apprentissage superviseÌ utiliseÌe principalement pour la classification. Il appartient aÌ€ la famille des meÌthodes dâ€™ensemble et fonctionne en combinant plusieurs classificateurs faibles (souvent des arbres de deÌcision de profondeur 1, appeleÌs stumps) pour creÌer un modeÌ€le puissant et robuste.

AdaBoost fonctionne en attribuant un poids aÌ€ chaque observation et en entraiÌ‚nant une seÌrie de classificateurs faibles de manieÌ€re iteÌrative. AÌ€ chaque iteÌration, les erreurs des modeÌ€les preÌceÌdents sont amplifieÌes : les observations mal classeÌes recÌ§oivent un poids plus eÌleveÌ pour que le modeÌ€le suivant se concentre davantage sur elles. La preÌdiction finale est obtenue par un vote pondeÌreÌ des classificateurs.

:::{image} ./../assets/Schematic-diagram-of-AdaBoost-algorithm.png
:width: 450px
:alt: Pipeline of model training
:::

### Avantages

- AmeÌliore la preÌcision en combinant plusieurs modeÌ€les faibles.
- Fonctionne bien sur des donneÌes bruiteÌes et complexes.
- Peut eÌ‚tre utiliseÌ avec diffeÌrents modeÌ€les de base (arbres, SVM, etc.).

### InconvÃ©nients

- Sensible aux donneÌes bruiteÌes et aux outliers, qui peuvent eÌ‚tre sur-appris.
- Peut eÌ‚tre lent si le nombre dâ€™iteÌrations est eÌleveÌ.
- NeÌcessite un bon choix de lâ€™algorithme de base pour eÌviterÂ lâ€™overfitting.

## Early Stopping AdaBoost

L'algorithme Early Stopping AdaBoost est une variante d'AdaBoost qui introduit un criteÌ€re dâ€™arreÌ‚t anticipeÌ pour eÌviter lâ€™overfitting. Dans la version classique d'AdaBoost, le modeÌ€le continue d'ajouter des classificateurs faibles jusqu'aÌ€ atteindre un nombre preÌdeÌfini dâ€™iteÌrations, meÌ‚me si la performance commence aÌ€ se deÌgrader sur les donneÌes de validation.

Early Stopping AdaBoost surveille la performance du modeÌ€le aÌ€ chaque iteÌration et arreÌ‚te l'entraiÌ‚nement lorsqu'une deÌgradation est deÌtecteÌe, geÌneÌralement en suivant l'erreur sur un ensemble de validation.

### Formule de l'arrÃªt prÃ©coce

$L_{\text{val}}(t^* + 1) > L_{\text{val}}(t^*)$

OÃ¹:
- $L_{\text{val}}(t)$ : Perte sur l'ensemble de validation Ã  l'itÃ©ration $t$
- $t^*$ : Meilleur nombre d'itÃ©rations trouvÃ©
- $t$ : Nombre d'itÃ©rations actuel

Cette modification rÃ©duit le risque de sur-apprentissage et accÃ©lÃ¨re lâ€™entraÃ®nement car il permet d'eÌviter une complexiteÌ inutile et aussi il ameÌliore la geÌneÌralisation sur de nouvelles donneÌes.

- $L_{\text{val}}(t)$ : ReprÃ©sente la **perte** du modÃ¨le sur lâ€™ensemble de **validation** Ã  lâ€™itÃ©ration $t$.
- $t$ : Correspond au **nombre d'itÃ©rations** d'AdaBoost, c'est-Ã -dire le nombre de classificateurs faibles ajoutÃ©s jusqu'Ã  prÃ©sent.
- $t^*$: DÃ©signe le **meilleur nombre dâ€™itÃ©rations trouvÃ©**, c'est-Ã -dire lâ€™itÃ©ration oÃ¹ la perte sur lâ€™ensemble de validation est **minimale**.

- **Condition dâ€™arrÃªt**: Si, Ã  lâ€™itÃ©ration $t^* + 1$, la perte $L_{\text{val}} $ augmente par rapport Ã  lâ€™itÃ©ration $t^*$, cela signifie que le modÃ¨le commence Ã  sur-apprendre les donnÃ©es d'entraÃ®nement, donc l'entraÃ®nement est stoppÃ© pour Ã©viter lâ€™overfitting.

### Avantages

- ReÌduit le risque d'overfitting en arreÌ‚tant l'entraiÌ‚nement au bon moment.
- Diminue le temps d'exeÌcution en eÌvitant des iteÌrations inutiles.
- Peut ameÌliorer la performance sur des donneÌes bruiteÌes.

### InconvÃ©nients

- NeÌcessite un ensemble de validation pour surveiller la performance.
- Le choix du criteÌ€re d'arreÌ‚t peut eÌ‚tre deÌlicat et neÌcessite des ajustements.
- Peut ne pas eÌ‚tre optimal si l'arreÌ‚t est deÌclencheÌ trop toÌ‚t, empeÌ‚chant le modeÌ€le d'atteindre son pleinÂ potentiel.