---
title: Approches non linÃ©aires
---
## Decision Tree

L'algorithme des arbres de deÌcision (Decision Tree) est une meÌthode d'apprentissage superviseÌ utiliseÌe pour la classification et la reÌgression. Il repose sur une structure arborescente ouÌ€ chaque nÅ“ud repreÌsente une question sur une caracteÌristique des donneÌes, chaque branche correspond aÌ€ une reÌponse possible, et chaque feuille donne une preÌdiction.

 ### Principe de fonctionnement :

1. CreÌation de l'arbre : L'algorithme divise les donneÌes en fonction de la caracteÌristique qui maximise la seÌparation des classes ou minimise l'erreur de preÌdiction. Ce choix se fait souvent avec des mesures comme l'entropie (pour l'indice de Gini ou l'information gain en classification) ou l'erreur quadratique moyenne (en reÌgression).


2. Parcours de l'arbre : Pour preÌdire une nouvelle observation, on part de la racine et on suit les branches en fonction des valeurs des caracteÌristiques jusqu'aÌ€ atteindre une feuille.


3. PreÌdiction :

Classification : L'eÌtiquette de la classe la plus freÌquente dans la feuille est assigneÌe.

ReÌgression : La valeur moyenne des observations contenues dans la feuille est utiliseÌe comme preÌdiction.



## ForÃªt AlÃ©atoire 

Lâ€™algorithme ForÃªt AlÃ©atoire(Random Forest en anglais) est une extension des arbres de deÌcision, utiliseÌe pour la classification et la reÌgression. Il repose sur un principe dâ€™apprentissage ensembliste (ensemble learning), combinant plusieurs arbres pour ameÌliorer la robustesse et la preÌcision des preÌdictions.

Principe de fonctionnement :

1. Construction dâ€™une foreÌ‚t dâ€™arbres de deÌcision

Lâ€™algorithme geÌneÌ€re plusieurs arbres en effectuant un eÌchantillonnage aleÌatoire avec remise sur le jeu de donneÌes (bootstrap).

AÌ€ chaque division dans un arbre, un sous-ensemble aleÌatoire de caracteÌristiques est seÌlectionneÌ pour eÌviter que tous les arbres se construisent de la meÌ‚me manieÌ€re.



2. PreÌdiction par agreÌgation des arbres

Classification : Chaque arbre vote pour une classe, et la classe majoritaire est retenue (vote majoritaire).

ReÌgression : La preÌdiction finale est la moyenne des preÌdictions des diffeÌrents arbres.



Avantages :

-plus prÃ©cis et gÃ©nÃ©ralisable sans risque de surapprentissage quâ€™un arbre unique.

-Peu sensible aux valeurs aberrantes graÌ‚ce aÌ€ lâ€™agreÌgation des arbres.

-robustesse : Une petite variation dans les donneÌes nâ€™impacte pas fortement le modeÌ€le.


-GeÌ€re bien les donneÌes manquantes, les grands ensembles de donneÌes graÌ‚ce aÌ€ la seÌlection aleÌatoire des caracteÌristiques et il fonctionne bien meÌ‚me avec de nombreuses variables.


InconveÌnients :

-Moins interpreÌtable quâ€™un arbre unique, car le reÌsultat final reÌsulte de plusieurs modeÌ€les.

-Temps de calcul plus long pour de grands ensembles de donneÌes.

-Performances limiteÌes sur des donneÌes treÌ€s haute dimension par rapport aÌ€ dâ€™autres modeÌ€les avanceÌs comme Adaboost, XGBoost ou LightGBM.
 
##  ForÃªt AlÃ©atoire Sensible au CoÃ»t

Lâ€™algorithme ForÃªt AlÃ©atoire Sensible au CoÃ»t (Cost-Sensitive Random Forest en anglais) est une adaptation du Random Forest concÌ§ue pour prendre en compte des couÌ‚ts diffeÌrents dâ€™erreur lors de la classification. Il est particulieÌ€rement utile lorsque les classes sont deÌseÌquilibreÌes ou que certaines erreurs ont un impact plus important que dâ€™autres, comme en dÃ©tection de fraude ou en diagnostic mÃ©dical.

Principe de fonctionnement :

1. PondeÌration des erreurs

Contrairement au Random Forest classique, qui minimise simplement le taux dâ€™erreur global, le Cost-Sensitive Random Forest attribue un couÌ‚t aÌ€ chaque type dâ€™erreur.

Une matrice de couÌ‚ts est deÌfinie pour peÌnaliser diffeÌremment les erreurs en fonction de leur importance.



2. Construction dâ€™une foreÌ‚t de deÌcision pondeÌreÌe

Comme dans Random Forest, plusieurs arbres sont geÌneÌreÌs sur des eÌchantillons bootstrap des donneÌes.

AÌ€ chaque division, le criteÌ€re de seÌlection (comme lâ€™indice de Gini ou lâ€™entropie) est ajusteÌ pour tenir compte des couÌ‚ts des erreurs.

$$ Gini_C(D) = \sum_{i=1}^{C} p_i \sum_{j=1}^{C} C_{ij} p_j $$ 

Le terme $ C_{ij}$ reprÃ©sente le coÃ»t associÃ© Ã  la classification erronÃ©e d'une observation appartenant Ã  la classe $ğ‘–$ en classe $ğ‘—$.

$C_{ii}=0$ : Il n'y a aucun coÃ»t lorsqu'une observation est correctement classÃ©e.

$ C_{ij} > 0$ pour $ğ‘–â‰ ğ‘—$: Il y a un coÃ»t lorsqu'une observation de la classe $ğ‘–$ est incorrectement classÃ©e comme appartenant Ã  la classe $ğ‘—$.

3. PreÌdiction avec prise en compte des couÌ‚ts

 PlutoÌ‚t que dâ€™utiliser un vote majoritaire simple entre les arbres, la classe preÌdite est celle qui minimise le couÌ‚t dâ€™erreur attendu.Cette modification rÃ©duit le biais envers les classes majoritaires et amÃ©liore la prise en compte des classes minoritaires.

Avantages :

Meilleure prise en compte des deÌseÌquilibres de classe: Dans un probleÌ€me ouÌ€ une classe est beaucoup moins freÌquente que lâ€™autre, Random Forest peut privileÌgier la classe majoritaire. En inteÌgrant un couÌ‚t plus eÌleveÌ pour les erreurs sur la classe minoritaire, lâ€™algorithme devient plus eÌquitable.



Plus adapteÌ aux contextes ouÌ€ certaines erreurs couÌ‚tent plus cher que dâ€™autres : Par exemple, dans une deÌtection de fraude bancaire, une fausse alerte (preÌdire une fraude inexistante) est moins grave quâ€™un faux neÌgatif (ne pas deÌtecter une fraude reÌelle).


Garde la robustesse du Random Forest tout en ameÌliorant la gestion des erreurs critiques.


InconveÌnients :

NeÌcessite de bien deÌfinir la matrice de couÌ‚ts, ce qui peut eÌ‚tre deÌlicat en lâ€™absence dâ€™informations preÌcises sur lâ€™impact des erreurs.

Peut eÌ‚tre plus long aÌ€ entraiÌ‚ner, car lâ€™arbre doit ajuster ses criteÌ€res de seÌlection en fonction des couÌ‚ts dâ€™erreur.

Moins intuitif que le Random Forest standard, car les deÌcisions ne sont plus baseÌes uniquement sur des votes majoritaires.


## AdaBoost (Adaptive Boosting)

L'algorithme AdaBoost (Adaptive Boosting) est une meÌthode dâ€™apprentissage superviseÌ utiliseÌe principalement pour la classification. Il appartient aÌ€ la famille des meÌthodes dâ€™ensemble et fonctionne en combinant plusieurs classificateurs faibles (souvent des arbres de deÌcision de profondeur 1, appeleÌs stumps) pour creÌer un modeÌ€le puissant et robuste.

AdaBoost fonctionne en attribuant un poids aÌ€ chaque observation et en entraiÌ‚nant une seÌrie de classificateurs faibles de manieÌ€re iteÌrative. AÌ€ chaque iteÌration, les erreurs des modeÌ€les preÌceÌdents sont amplifieÌes : les observations mal classeÌes recÌ§oivent un poids plus eÌleveÌ pour que le modeÌ€le suivant se concentre davantage sur elles. La preÌdiction finale est obtenue par un vote pondeÌreÌ des classificateurs.

:::{image}  C:\Users\HP\Downloads\guillaume\classification-experiments\assets\Schematic-diagram-of-AdaBoost-algorithm.png
:width: 450px
:alt: Pipeline of model training
:::

Avantages :

-AmeÌliore la preÌcision en combinant plusieurs modeÌ€les faibles.

-Fonctionne bien sur des donneÌes bruiteÌes et complexes.

-Peut eÌ‚tre utiliseÌ avec diffeÌrents modeÌ€les de base (arbres, SVM, etc.).


InconveÌnients :

-Sensible aux donneÌes bruiteÌes et aux outliers, qui peuvent eÌ‚tre sur-appris.

-Peut eÌ‚tre lent si le nombre dâ€™iteÌrations est eÌleveÌ.

-NeÌcessite un bon choix de lâ€™algorithme de base pour eÌviterÂ lâ€™overfitting.


## Early Stopping AdaBoost

L'algorithme Early Stopping AdaBoost est une variante d'AdaBoost qui introduit un criteÌ€re dâ€™arreÌ‚t anticipeÌ pour eÌviter lâ€™overfitting. Dans la version classique d'AdaBoost, le modeÌ€le continue d'ajouter des classificateurs faibles jusqu'aÌ€ atteindre un nombre preÌdeÌfini dâ€™iteÌrations, meÌ‚me si la performance commence aÌ€ se deÌgrader sur les donneÌes de validation.

Early Stopping AdaBoost surveille la performance du modeÌ€le aÌ€ chaque iteÌration et arreÌ‚te l'entraiÌ‚nement lorsqu'une deÌgradation est deÌtecteÌe, geÌneÌralement en suivant l'erreur sur un ensemble de validation.

$$ L_{\text{val}}(t^* + 1) > L_{\text{val}}(t^*) $$

 Cette modification rÃ©duit le risque de sur-apprentissage et accÃ©lÃ¨re lâ€™entraÃ®nement car il permet d'eÌviter une complexiteÌ inutile et aussi il  ameÌliore la geÌneÌralisation sur de nouvelles donneÌes.

- **\( L_{\text{val}}(t) \)** : ReprÃ©sente la **perte**  du modÃ¨le sur lâ€™ensemble de **validation** Ã  lâ€™itÃ©ration \( t \).

- **\( t \)** : Correspond au **nombre d'itÃ©rations** d'AdaBoost, c'est-Ã -dire le nombre de classificateurs faibles ajoutÃ©s jusqu'Ã  prÃ©sent.

- **\( t^* \)** : DÃ©signe le **meilleur nombre dâ€™itÃ©rations trouvÃ©**, c'est-Ã -dire lâ€™itÃ©ration oÃ¹ la perte sur lâ€™ensemble de validation est **minimale**.

- **Condition dâ€™arrÃªt** : Si, Ã  lâ€™itÃ©ration \( t^* + 1 \), la perte \( L_{\text{val}} \)augmente par rapport Ã  lâ€™itÃ©ration \( t^* \), cela signifie que le modÃ¨le commence Ã  sur-apprendre les donnÃ©es d'entraÃ®nement, donc l'entraÃ®nement est stoppÃ© pour Ã©viter lâ€™overfitting.


Avantages :

-ReÌduit le risque d'overfitting en arreÌ‚tant l'entraiÌ‚nement au bon moment.

-Diminue le temps d'exeÌcution en eÌvitant des iteÌrations inutiles.

-Peut ameÌliorer la performance sur des donneÌes bruiteÌes.


InconveÌnients :

-NeÌcessite un ensemble de validation pour surveiller la performance.

-Le choix du criteÌ€re d'arreÌ‚t peut eÌ‚tre deÌlicat et neÌcessite des ajustements.

-Peut ne pas eÌ‚tre optimal si l'arreÌ‚t est deÌclencheÌ trop toÌ‚t, empeÌ‚chant le modeÌ€le d'atteindre son pleinÂ potentiel.