---
title: Conclusion
---

## Tableau récapitulatif des algorithmes

| Algorithme | Type | Principe | Avantages | Inconvénients | Complexité | Hyperparamètres clés |
|------------|------|----------|-----------|---------------|------------|----------------------|
| **KNN** | Non paramétrique | Classification/régression basée sur les k plus proches voisins d'un point | - Simple à comprendre et à implémenter<br>- Pas de phase d'entraînement<br>- Adapté aux frontières de décision complexes | - Coûteux en prédiction<br>- Sensible à l'échelle des données<br>- Performances réduites en haute dimension | O(nd) pour chaque prédiction où n = nombre d'exemples, d = dimensions | - Nombre de voisins k<br>- Métrique de distance |
| **Distance Weighted KNN** | Non paramétrique | Variante de KNN où les voisins les plus proches ont plus d'influence sur la prédiction | - Meilleure prise en compte de la proximité<br>- Plus robuste que KNN standard<br>- Réduit l'impact des valeurs aberrantes | - Plus complexe à mettre en œuvre<br>- Toujours coûteux en prédiction<br>- Choix de la fonction de poids | O(nd) pour chaque prédiction | - Nombre de voisins k<br>- Fonction de poids<br>- Métrique de distance |
| **CNN (Condensed Nearest Neighbor)** | Non paramétrique | Sélectionne un sous-ensemble représentatif des données d'entraînement | - Réduit le stockage et le temps de prédiction<br>- Élimine les exemples redondants<br>- Garde les exemples à la frontière de décision | - Sensible à l'ordre des données<br>- Peut avoir du mal avec des données bruitées<br>- Performance variable | O(n²d) pour la condensation | - Métrique de distance<br>- Seuil de sélection |
| **LANN (Local Average of Nearest Neighbors)** | Non paramétrique | Utilise la moyenne locale des k plus proches voisins pour faire des prédictions | - Robuste au bruit<br>- Meilleure généralisation que KNN standard<br>- Bonne performance sur les données continues | - Moins efficace sur les données catégorielles<br>- Toujours coûteux en calcul<br>- Perte d'interprétabilité | O(nd) pour chaque prédiction | - Nombre de voisins k<br>- Schéma de pondération |
| **Régression logistique** | Paramétrique | Modèle linéaire pour la classification utilisant la fonction logistique pour estimer les probabilités | - Simple et interprétable<br>- Efficace pour les grandes données<br>- Donne des probabilités<br>- Faible variance | - Limité aux frontières linéaires<br>- Suppose l'indépendance des variables<br>- Sensible aux valeurs aberrantes | O(nd) pour l'entraînement | - Régularisation (C ou λ)<br>- Seuil de décision |
| **SVM (Support Vector Machine)** | Paramétrique | Trouve l'hyperplan qui maximise la marge entre les classes | - Efficace en haute dimension<br>- Robuste avec le kernel trick<br>- Bonne généralisation<br>- Peu sensible au surapprentissage | - Difficulté à interpréter<br>- Choix du kernel crucial<br>- Lent sur de grandes données<br>- Difficile à calibrer pour les probabilités | O(n²) à O(n³) selon l'implémentation | - Paramètre C<br>- Type de kernel<br>- Paramètres du kernel |
| **Arbres de décision** | Non linéaire | Crée un modèle de décision basé sur des règles apprises à partir des données | - Facile à comprendre et visualiser<br>- Gère les données mixtes<br>- Peu sensible à l'échelle des variables<br>- Capture les interactions non linéaires | - Tendance au surapprentissage<br>- Instable (haute variance)<br>- Biais pour certaines classes<br>- Difficulté avec les relations linéaires | O(n log n) pour l'entraînement | - Profondeur maximale<br>- Critère de division<br>- Nombre min. d'échantillons par feuille |
| **Random Forest** | Non linéaire (ensemble) | Combine plusieurs arbres de décision entraînés sur des sous-ensembles aléatoires | - Réduit le surapprentissage<br>- Robuste au bruit et aux valeurs aberrantes<br>- Fournit l'importance des variables<br>- Parallélisable | - Moins interprétable qu'un seul arbre<br>- Plus lent à entraîner<br>- Biaisé sur des données déséquilibrées<br>- Utilisation intensive de mémoire | O(K·n log n) où K = nombre d'arbres | - Nombre d'arbres<br>- Nombre de caractéristiques par division<br>- Profondeur max des arbres |
| **AdaBoost** | Non linéaire (ensemble) | Ensemble séquentiel qui donne plus de poids aux exemples mal classifiés | - Bonne généralisation<br>- Simple à implémenter<br>- Sélection automatique des caractéristiques<br>- Peu de paramètres à régler | - Sensible aux valeurs aberrantes<br>- Sensible au bruit<br>- Peut surapprendre avec trop d'itérations<br>- Moins efficace que d'autres ensembles sur certains problèmes | O(K·n log n) où K = nombre d'itérations | - Nombre d'estimateurs<br>- Taux d'apprentissage<br>- Type de classificateur faible |

Pour des problèmes complexes avec suffisamment de données, les méthodes d'ensemble comme le bagging surpassent généralement les modèles individuels comme la régression logistique ou même les SVM. Cependant, lorsque l'interprétabilité est cruciale ou les données limitées, la régression logistique peut être préférable. Les SVM offrent un bon compromis entre complexité et performance, particulièrement adaptés aux cas où la frontière de décision est non linéaire mais bien définie.

Les performances observés sont cohérentes avec les publications sur le sujet (<https://doi.org/10.48550/arXiv.2207.08815>).