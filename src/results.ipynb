{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Évaluation des classifieurs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce document présente les résultats des différents classifieurs binaire de façon aggrégé par rapport à leurs performance respectives sur les 28 jeux de données fournis. \n",
    "\n",
    "La phase d'optimisation des hyperparamètres a été réalisée pour chaque classifieur en utilisant un `RandomHalvingRandomSearchCV` pour réduire le temps d'exécution au lieu d'un classique `GridSearchCV`. Toutefois, en contrepartie, cela a généré des erreurs de fit pour certains classifieurs.\n",
    "\n",
    "Nous pensons que ces erreurs de fit sont liés au paramètre de ressources minimale du `RandomHalvingRandomSearchCV` qui réduit le nombre de sample pour l'entrainement et peut malencontreusement fit le modèle sur des données ne contenant qu'une unique classe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Présentation des jeux de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_samples</th>\n",
       "      <th>n_features</th>\n",
       "      <th>majority_class_proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abalone8</th>\n",
       "      <td>4177</td>\n",
       "      <td>10</td>\n",
       "      <td>86%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abalone17</th>\n",
       "      <td>4177</td>\n",
       "      <td>10</td>\n",
       "      <td>99%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abalone20</th>\n",
       "      <td>4177</td>\n",
       "      <td>10</td>\n",
       "      <td>99%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autompg</th>\n",
       "      <td>392</td>\n",
       "      <td>7</td>\n",
       "      <td>62%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>australian</th>\n",
       "      <td>690</td>\n",
       "      <td>14</td>\n",
       "      <td>56%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balance</th>\n",
       "      <td>625</td>\n",
       "      <td>4</td>\n",
       "      <td>54%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bankmarketing</th>\n",
       "      <td>45211</td>\n",
       "      <td>51</td>\n",
       "      <td>88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bupa</th>\n",
       "      <td>345</td>\n",
       "      <td>6</td>\n",
       "      <td>58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>german</th>\n",
       "      <td>1000</td>\n",
       "      <td>24</td>\n",
       "      <td>70%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glass</th>\n",
       "      <td>214</td>\n",
       "      <td>9</td>\n",
       "      <td>67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hayes</th>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "      <td>77%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart</th>\n",
       "      <td>270</td>\n",
       "      <td>13</td>\n",
       "      <td>56%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iono</th>\n",
       "      <td>351</td>\n",
       "      <td>34</td>\n",
       "      <td>64%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>libras</th>\n",
       "      <td>360</td>\n",
       "      <td>90</td>\n",
       "      <td>93%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newthyroid</th>\n",
       "      <td>215</td>\n",
       "      <td>5</td>\n",
       "      <td>70%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pageblocks</th>\n",
       "      <td>5473</td>\n",
       "      <td>10</td>\n",
       "      <td>90%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pima</th>\n",
       "      <td>768</td>\n",
       "      <td>8</td>\n",
       "      <td>65%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>satimage</th>\n",
       "      <td>6435</td>\n",
       "      <td>36</td>\n",
       "      <td>90%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>segmentation</th>\n",
       "      <td>2310</td>\n",
       "      <td>19</td>\n",
       "      <td>86%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sonar</th>\n",
       "      <td>208</td>\n",
       "      <td>60</td>\n",
       "      <td>53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spambase</th>\n",
       "      <td>4597</td>\n",
       "      <td>57</td>\n",
       "      <td>61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>splice</th>\n",
       "      <td>3175</td>\n",
       "      <td>60</td>\n",
       "      <td>52%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vehicle</th>\n",
       "      <td>846</td>\n",
       "      <td>18</td>\n",
       "      <td>76%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wdbc</th>\n",
       "      <td>569</td>\n",
       "      <td>30</td>\n",
       "      <td>63%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine</th>\n",
       "      <td>178</td>\n",
       "      <td>13</td>\n",
       "      <td>67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine4</th>\n",
       "      <td>1599</td>\n",
       "      <td>11</td>\n",
       "      <td>97%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeast3</th>\n",
       "      <td>1484</td>\n",
       "      <td>8</td>\n",
       "      <td>89%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeast6</th>\n",
       "      <td>1484</td>\n",
       "      <td>8</td>\n",
       "      <td>98%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               n_samples  n_features majority_class_proportion\n",
       "dataset                                                       \n",
       "abalone8            4177          10                       86%\n",
       "abalone17           4177          10                       99%\n",
       "abalone20           4177          10                       99%\n",
       "autompg              392           7                       62%\n",
       "australian           690          14                       56%\n",
       "balance              625           4                       54%\n",
       "bankmarketing      45211          51                       88%\n",
       "bupa                 345           6                       58%\n",
       "german              1000          24                       70%\n",
       "glass                214           9                       67%\n",
       "hayes                132           4                       77%\n",
       "heart                270          13                       56%\n",
       "iono                 351          34                       64%\n",
       "libras               360          90                       93%\n",
       "newthyroid           215           5                       70%\n",
       "pageblocks          5473          10                       90%\n",
       "pima                 768           8                       65%\n",
       "satimage            6435          36                       90%\n",
       "segmentation        2310          19                       86%\n",
       "sonar                208          60                       53%\n",
       "spambase            4597          57                       61%\n",
       "splice              3175          60                       52%\n",
       "vehicle              846          18                       76%\n",
       "wdbc                 569          30                       63%\n",
       "wine                 178          13                       67%\n",
       "wine4               1599          11                       97%\n",
       "yeast3              1484           8                       89%\n",
       "yeast6              1484           8                       98%"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../src/\")\n",
    "\n",
    "from get_dataset import dataset_loaders\n",
    "import pandas as pd\n",
    "\n",
    "datasets = dataset_loaders.keys()\n",
    "\n",
    "data = []\n",
    "for dataset_name in datasets:\n",
    "    X, y = dataset_loaders[dataset_name]()\n",
    "    n_samples, n_features = X.shape\n",
    "    majority_class_proportion = max(sum(y == cls) for cls in set(y)) / n_samples\n",
    "    majority_class_proportion = \"{:.0%}\".format(majority_class_proportion)\n",
    "    data.append([dataset_name, n_samples, n_features, majority_class_proportion])\n",
    "    \n",
    "df = pd.DataFrame(data, columns=[\"dataset\", \"n_samples\", \"n_features\", \"majority_class_proportion\"])\n",
    "df = df.set_index(\"dataset\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Évaluation des classifieurs binaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Harmonic Mean Accuracy</th>\n",
       "      <th>Harmonic Mean F1 Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 2, ...</td>\n",
       "      <td>0.907637</td>\n",
       "      <td>0.900563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest - cost-sensitive learning</th>\n",
       "      <td>{'n_estimators': 100, 'min_samples_split': 10,...</td>\n",
       "      <td>0.892766</td>\n",
       "      <td>0.895531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>{'n_estimators': 100, 'learning_rate': 1.0}</td>\n",
       "      <td>0.893424</td>\n",
       "      <td>0.886018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 7, 'loss': ...</td>\n",
       "      <td>0.884921</td>\n",
       "      <td>0.878145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>{'penalty': 'l1', 'class_weight': 'balanced', ...</td>\n",
       "      <td>0.877503</td>\n",
       "      <td>0.871661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>{'degree': 4, 'C': 1}</td>\n",
       "      <td>0.877779</td>\n",
       "      <td>0.869084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>{'min_samples_split': 10, 'max_depth': 7}</td>\n",
       "      <td>0.863375</td>\n",
       "      <td>0.860329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM non linéaire avec SMOTE</th>\n",
       "      <td>{'svm__kernel': 'sigmoid', 'svm__gamma': 'auto...</td>\n",
       "      <td>0.849529</td>\n",
       "      <td>0.857669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN Condensed Nearest Neighbor</th>\n",
       "      <td>{'knn__n_neighbors': 3, 'cnn__n_neighbors': 7}</td>\n",
       "      <td>0.862886</td>\n",
       "      <td>0.855076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>{'n_neighbors': 5}</td>\n",
       "      <td>0.859005</td>\n",
       "      <td>0.852526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN Distance Weighted</th>\n",
       "      <td>{'n_neighbors': 5}</td>\n",
       "      <td>0.858057</td>\n",
       "      <td>0.850419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN Locally Adaptive</th>\n",
       "      <td>{'n_neighbors': 9}</td>\n",
       "      <td>0.857293</td>\n",
       "      <td>0.847385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM cost-sensitive learning</th>\n",
       "      <td>{'kernel': 'sigmoid', 'gamma': 'auto', 'C': 1}</td>\n",
       "      <td>0.834354</td>\n",
       "      <td>0.842828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM non linéaire</th>\n",
       "      <td>{'kernel': 'rbf', 'gamma': 'scale', 'C': 1}</td>\n",
       "      <td>0.859435</td>\n",
       "      <td>0.828005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>-</td>\n",
       "      <td>0.869434</td>\n",
       "      <td>0.863488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                Parameters  \\\n",
       "Model Name                                                                                   \n",
       "Random Forest                            {'n_estimators': 200, 'min_samples_split': 2, ...   \n",
       "Random Forest - cost-sensitive learning  {'n_estimators': 100, 'min_samples_split': 10,...   \n",
       "AdaBoost                                       {'n_estimators': 100, 'learning_rate': 1.0}   \n",
       "Gradient Boosting                        {'n_estimators': 100, 'max_depth': 7, 'loss': ...   \n",
       "Logistic Regression                      {'penalty': 'l1', 'class_weight': 'balanced', ...   \n",
       "SVM                                                                  {'degree': 4, 'C': 1}   \n",
       "Decision Tree                                    {'min_samples_split': 10, 'max_depth': 7}   \n",
       "SVM non linéaire avec SMOTE              {'svm__kernel': 'sigmoid', 'svm__gamma': 'auto...   \n",
       "KNN Condensed Nearest Neighbor              {'knn__n_neighbors': 3, 'cnn__n_neighbors': 7}   \n",
       "KNN                                                                     {'n_neighbors': 5}   \n",
       "KNN Distance Weighted                                                   {'n_neighbors': 5}   \n",
       "KNN Locally Adaptive                                                    {'n_neighbors': 9}   \n",
       "SVM cost-sensitive learning                 {'kernel': 'sigmoid', 'gamma': 'auto', 'C': 1}   \n",
       "SVM non linéaire                               {'kernel': 'rbf', 'gamma': 'scale', 'C': 1}   \n",
       "Average                                                                                  -   \n",
       "\n",
       "                                         Harmonic Mean Accuracy  \\\n",
       "Model Name                                                        \n",
       "Random Forest                                          0.907637   \n",
       "Random Forest - cost-sensitive learning                0.892766   \n",
       "AdaBoost                                               0.893424   \n",
       "Gradient Boosting                                      0.884921   \n",
       "Logistic Regression                                    0.877503   \n",
       "SVM                                                    0.877779   \n",
       "Decision Tree                                          0.863375   \n",
       "SVM non linéaire avec SMOTE                            0.849529   \n",
       "KNN Condensed Nearest Neighbor                         0.862886   \n",
       "KNN                                                    0.859005   \n",
       "KNN Distance Weighted                                  0.858057   \n",
       "KNN Locally Adaptive                                   0.857293   \n",
       "SVM cost-sensitive learning                            0.834354   \n",
       "SVM non linéaire                                       0.859435   \n",
       "Average                                                0.869434   \n",
       "\n",
       "                                         Harmonic Mean F1 Score  \n",
       "Model Name                                                       \n",
       "Random Forest                                          0.900563  \n",
       "Random Forest - cost-sensitive learning                0.895531  \n",
       "AdaBoost                                               0.886018  \n",
       "Gradient Boosting                                      0.878145  \n",
       "Logistic Regression                                    0.871661  \n",
       "SVM                                                    0.869084  \n",
       "Decision Tree                                          0.860329  \n",
       "SVM non linéaire avec SMOTE                            0.857669  \n",
       "KNN Condensed Nearest Neighbor                         0.855076  \n",
       "KNN                                                    0.852526  \n",
       "KNN Distance Weighted                                  0.850419  \n",
       "KNN Locally Adaptive                                   0.847385  \n",
       "SVM cost-sensitive learning                            0.842828  \n",
       "SVM non linéaire                                       0.828005  \n",
       "Average                                                0.863488  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.stats import hmean\n",
    "\n",
    "results_files = list(Path(\"./../results\").rglob(\"*.joblib\"))\n",
    "\n",
    "combined_results = {}\n",
    "\n",
    "for file in results_files:\n",
    "    results = joblib.load(file)\n",
    "    for model_name, result in results.items():\n",
    "        if model_name not in combined_results:\n",
    "            combined_results[model_name] = {\n",
    "                'best_params': result['best_params'],\n",
    "                'accuracies': [],\n",
    "                'f1_scores': []\n",
    "            }\n",
    "        y_true = result['y_true']\n",
    "        y_pred = result['y_pred']\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        combined_results[model_name]['accuracies'].append(accuracy)\n",
    "        combined_results[model_name]['f1_scores'].append(f1)\n",
    "\n",
    "model_names = []\n",
    "params = []\n",
    "harmonic_means = []\n",
    "\n",
    "for model_name, result in combined_results.items():\n",
    "    accuracies = result['accuracies']\n",
    "    f1_scores = result['f1_scores']\n",
    "    \n",
    "    # Moyenne harmonique des métriques par rapport à chaque jeu de donnée\n",
    "    harmonic_mean_accuracy = hmean(accuracies)\n",
    "    harmonic_mean_f1 = hmean(f1_scores)\n",
    "\n",
    "    model_names.append(model_name)\n",
    "    params.append(result['best_params'])\n",
    "    harmonic_means.append((harmonic_mean_accuracy, harmonic_mean_f1))\n",
    "    \n",
    "df = pd.DataFrame({\n",
    "    'Model Name': model_names,\n",
    "    'Parameters': params,\n",
    "    'Harmonic Mean Accuracy': [hm[0] for hm in harmonic_means],\n",
    "    'Harmonic Mean F1 Score': [hm[1] for hm in harmonic_means]\n",
    "})\n",
    "\n",
    "df = df.sort_values(by='Harmonic Mean F1 Score', ascending=False)\n",
    "\n",
    "average_harmonic_mean_accuracy = hmean(df['Harmonic Mean Accuracy'])\n",
    "average_harmonic_mean_f1_score = hmean(df['Harmonic Mean F1 Score'])\n",
    "df.loc['Average'] = ['Average', '-', average_harmonic_mean_accuracy, average_harmonic_mean_f1_score]\n",
    "\n",
    "df = df.set_index('Model Name')\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En examinant les résultats, on remarque que le Random Forest est l’un des modèles les plus performants. Sa version avec un apprentissage coût-sensible est étonnament moins efficace sur la base du score F1.\n",
    "\n",
    "Les méthodes de boosting, comme AdaBoost et Gradient Boosting, affichent également de bons résultats, bien qu’elles restent légèrement en retrait par rapport au Random Forest. Ces modèles, bien que puissants, peuvent être plus sensibles aux hyperparamètres et aux bruits dans les données.\n",
    "\n",
    "Les modèles linéaires, tels que la régression logistique et le SVM linéaire, obtiennent des performances légèrement inférieures aux méthodes à base d’arbres. La régression logistique reste compétitive et peut être une bonne alternative lorsqu’une meilleure interprétabilité est recherchée. Le SVM linéaire, quant à lui, est légèrement en retrait, bien qu’il conserve une certaine robustesse.\n",
    "\n",
    "Concernant les modèles non linéaires, comme le SVM avec des noyaux non linéaires et les différentes variantes du KNN, leurs performances ne surpassent pas celles des forêts d’arbres. Le SVM utilisant SMOTE semble particulièrement affecté par une perte de performance. Le KNN, qu’il soit pondéré ou adaptatif, ne se démarque pas de manière significative et reste globalement moins efficace que les méthodes d’ensemble et les modèles linéaires.\n",
    "\n",
    "De manière générale, les modèles basés sur des arbres, comme Random Forest et Gradient Boosting, semblent offrir le meilleur compromis entre performance et robustesse. Les modèles linéaires, bien que légèrement moins efficaces, restent des alternatives intéressantes, notamment pour leur interprétabilité. Les méthodes basées sur la proximité, comme KNN, et les SVM non linéaires ne semblent pas apporter d’amélioration significative et pourraient être moins adaptées dans ce contexte."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
