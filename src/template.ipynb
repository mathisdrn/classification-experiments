{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Implémentation de classifieurs binaires \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from utils import roc_plot, precision_recall_plot, table_report\n",
    "from get_dataset import load_dataset, dataset_loaders\n",
    "\n",
    "dataset = list(dataset_loaders.keys())[0]\n",
    "\n",
    "X, y = load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{dataset} contains n = {len(X)} samples with d = {X.shape[1]} features\n",
    "{y.mean() * 100:.2f}% of the samples are positive\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifieurs non paramétriques linéaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "y_pred_proba_knn = knn.predict_proba(X_test)[:,1]\n",
    "\n",
    "roc_plot(y_test, y_pred_proba_knn)\n",
    "precision_recall_plot(y_test, y_pred_proba_knn)\n",
    "table_report(y_test, y_pred_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance-Weighted KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Condensed Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locally Adaptive KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Distance-Weighted KNN\n",
    "weighted_knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "weighted_knn.fit(X_train, y_train)\n",
    "y_pred_weighted_knn = weighted_knn.predict(X_test)\n",
    "print(f\"Distance-Weighted KNN Accuracy: {accuracy_score(y_test, y_pred_weighted_knn)}\")\n",
    "\n",
    "# Locally Adaptive KNN (using a custom approach)\n",
    "class LocallyAdaptiveKNN(KNeighborsClassifier):\n",
    "    def predict(self, X):\n",
    "        distances, indices = self.kneighbors(X)\n",
    "        predictions = []\n",
    "        for i, neighbors in enumerate(indices):\n",
    "            local_k = int(len(neighbors) / 2)  # Example of adapting k locally\n",
    "            local_knn = KNeighborsClassifier(n_neighbors=local_k)\n",
    "            local_knn.fit(self._fit_X[neighbors], self._y[neighbors])\n",
    "            predictions.append(local_knn.predict([X[i]])[0])\n",
    "        return predictions\n",
    "\n",
    "adaptive_knn = LocallyAdaptiveKNN(n_neighbors=10)\n",
    "adaptive_knn.fit(X_train, y_train)\n",
    "y_pred_adaptive_knn = adaptive_knn.predict(X_test)\n",
    "print(f\"Locally Adaptive KNN Accuracy: {accuracy_score(y_test, y_pred_adaptive_knn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifieurs binaires non linéaires\n",
    "\n",
    "Algorithmes de classification non linéaires implémentés :\n",
    "- Arbres de décisions\n",
    "- Forêts aléatoires\n",
    "- Adaboost\n",
    "\n",
    "Arbres/Forêts :\n",
    "- Random Forest avec cost-sensitive learning\n",
    "- Extremely Randomized Trees\n",
    "- Gradient Boosted Decision Trees\n",
    "- Weighted Random Forest pour classes déséquilibrées\n",
    "\n",
    "AdaBoost :\n",
    "- AdaBoost avec différents classifieurs de base\n",
    "- Cost-sensitive AdaBoost\n",
    "- AdaBoost.M1 avec early stopping\n",
    "- RUSBoost (combine boosting et under-sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifieurs binaires paramétriques\n",
    "\n",
    "Algorithmes de classification paramétriques implémentés :\n",
    "- SVM linéaire (ou noyau linéaire)\n",
    "- Régression logistique\n",
    "\n",
    "SVM :\n",
    "- One-class SVM pour gérer le déséquilibre des classes\n",
    "- Combiner avec des méthodes de sous/sur-échantillonnage (SMOTE, RandomUnderSampling)\n",
    "- Cost-sensitive SVM : Différentes pénalités C pour chaque classe\n",
    "- Ensemble de SVMs avec bagging\n",
    "\n",
    "Régression logistique :\n",
    "- Régression logistique avec pénalisation élastique (combinaison L1/L2)\n",
    "- Cost-sensitive avec pondération des classes\n",
    "- Régression logistique polynomiale\n",
    "- Régression logistique avec sélection de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import roc_plot, precision_recall_plot, table_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La [](#table_report_LR1) montre les résultats de la classification par le modèle de régression logistique. On observe que :\n",
    "\n",
    "- $83,04 \\%$ des *spams* sont correctement identifiés\n",
    "- $99,59 \\%$ des *hams* sont correctement identifiés\n",
    "- $96,88 \\%$ des observations classifiées en tant que *spam* sont des *spams*\n",
    "- $97,43 \\%$ des observations classifiées en tant que *ham* sont des *hams*\n",
    "- Le score F1 moyen pondéré est de $97,98 \\%$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
